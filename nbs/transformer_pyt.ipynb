{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/Users/shwetank/code/makemore-utils-nbs')\n",
    "from utils import create_dataset, CharDataset, evaluate_loss, print_samples, get_lr_loss\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import torch\n",
    "from models import Xformer_Scratch as Xformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Device toggle\n",
    "device = torch.device(\"mps\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and create training and test sets\n",
    "names, vocab, max_length = create_dataset('../names.txt')\n",
    "vocab_size = len(vocab) + 1 # Blank token\n",
    "block_length = max_length + 1\n",
    "# print(len(names), vocab, max_length)\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "n1 = int(0.9*len(names))\n",
    "random.shuffle(names)\n",
    "# print(names[0:5], names_ss[0:5]) # check that dataset is shuffled\n",
    "train_data = CharDataset(names[:n1], vocab=vocab, max_length=max_length)\n",
    "test_data = CharDataset(names[n1:], vocab=vocab, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Hyperparameters\n",
    "emb_dim = 64\n",
    "num_heads = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Test run on a batch size of 1 for debug\n",
    "# batch_size = 1\n",
    "# train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# model = Xformer(emb_dim,vocab_size, num_heads, dropout).to(device)\n",
    "# optimizer = Adam(model.parameters(), lr=0.01)\n",
    "# xb, yb = next(iter(train_dataloader))\n",
    "# xb = xb.to(device)\n",
    "# yb = yb.to(device)\n",
    "# # print(xb.shape, yb.shape)\n",
    "# logits, loss = model(xb,yb)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4623, device='mps:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Set up optimizer and model\n",
    "batch_size = 512\n",
    "num_epochs = 100\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "model = Xformer(emb_dim,vocab_size, num_heads, dropout).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "xb, yb = next(iter(train_dataloader))\n",
    "xb = xb.to(device)\n",
    "yb = yb.to(device)\n",
    "logits, loss = model(xb,yb)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Optimal lr sweep\n",
    "# import matplotlib.pyplot as plt\n",
    "# lri, lossi =  get_lr_loss(model, optimizer, train_dataloader, num_epochs, device, -2, -1.5)\n",
    "# plt.plot(lri, lossi)\n",
    "# # Add labels to the x-axis and y-axis\n",
    "# plt.xlabel('LR (Learning Rate)')\n",
    "# plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop - Initialized in separate loop so that it can be re run without reinitialization that will wipe things out\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "tr_loss = []\n",
    "te_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop \n",
    "lr = 0.001\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "n_epochs = 1000\n",
    "for steps in range(n_epochs):\n",
    "    xb, yb = next(iter(train_dataloader))\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = model(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    tr_lossi, te_lossi = evaluate_loss(model, train_dataloader, test_dataloader, device, num_batches=10)\n",
    "    tr_loss.append(tr_lossi)\n",
    "    te_loss.append(te_lossi)\n",
    "    # print(loss.item())\n",
    "\n",
    "    # if steps % 99 == 0:\n",
    "    #     print('ltr: ', ltr, 'lte: ', lte, 'single shot loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot loss \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(tr_loss[10:], label='Training Loss')\n",
    "plt.plot(te_loss[10:], label='Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('training loss: ', round(torch.mean(torch.tensor(tr_loss[-100:])).item(),4)), \n",
    "print('validation loss: ', round((torch.mean(torch.tensor(te_loss[-100:]))).item(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pretty print the samples\n",
    "print_samples(model, train_data, max_length, device, num=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot tsne or pca for embeddings\n",
    "# t-SNE example:\n",
    "embeddings_matrix = model.token_embedding.weight.data.cpu().numpy()\n",
    "tsne = TSNE(n_components=2, perplexity=7)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings_matrix)\n",
    "\n",
    "# Create vectors for ticklabels\n",
    "ticklabels = list(train_data.itos.values())\n",
    "ticklabels.insert(0,'stop')\n",
    "ticklabels.insert(27,'blank')\n",
    "x_ticklabel_vec = np.arange(len(ticklabels))\n",
    "y_ticklabel_vec = np.arange(len(ticklabels))\n",
    "print(ticklabels)\n",
    "\n",
    "# visualize dimensions 0 and 1 of the embedding matrix for all characters\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(reduced_embeddings[:,0].data, reduced_embeddings[:,1].data, s=200)\n",
    "for i in range(embeddings_matrix.shape[0]):\n",
    "    plt.text(reduced_embeddings[i,0].item(), reduced_embeddings[i,1].item(), ticklabels[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
