{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/Users/shwetank/code/makemore-utils-nbs')\n",
    "from utils import create_dataset, CharDataset, evaluate_loss, print_samples, get_lr_loss\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and create training and test sets\n",
    "names, vocab, max_length = create_dataset('../names.txt')\n",
    "# print(len(names), vocab, max_length)\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "n1 = int(0.9*len(names))\n",
    "random.shuffle(names)\n",
    "# print(names[0:5], names_ss[0:5]) # check that dataset is shuffled\n",
    "train_data = CharDataset(names[:n1], vocab=vocab, max_length=max_length)\n",
    "test_data = CharDataset(names[n1:], vocab=vocab, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Hyperparameters\n",
    "vocab_size = len(vocab) + 1 # +1 for special characters\n",
    "embedding_dimension = 2\n",
    "hidden_dimension = 100\n",
    "block_length = max_length + 1 \n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self,vocab_size):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.wte = nn.Embedding(vocab_size + 1, embedding_dimension)\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(embedding_dimension*block_length, hidden_dimension),\n",
    "#             # nn.BatchNorm1d(hidden_dimension),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(hidden_dimension, vocab_size + 1)\n",
    "#         )\n",
    "\n",
    "#     # Rewrite forward pass to make it more intuitive - this is clever but not intuitive\n",
    "#     def forward(self, x, targets = None):\n",
    "#         # input to the neural network is x of dimension batch_size, time_index\n",
    "#         embs = []\n",
    "#         for k in range(block_length):\n",
    "#             tok_emb = self.wte(x) # batch_size, time_index, embedding_dimension\n",
    "#             # this rotates the matrix to make last element of the column the first\n",
    "#             x = torch.roll(x, 1, 1)\n",
    "#             # this sets the first column to a special <BLANK> token making the entire operation causal again. You update this to <BLANK> as many times as you rotate there by keeing things causal \n",
    "#             x[:, 0] = vocab_size \n",
    "#             embs.append(tok_emb)\n",
    "\n",
    "#         # print(x)\n",
    "#         x = torch.cat(embs, -1) # batch_size, time_index, embedding_dimension x block_length\n",
    "#         logits = self.mlp(x)    # batch_size, time_index, vocab_size + 1. \n",
    "\n",
    "#         # We need to align targets and logits to calculate cross entropy. targets.view(-1) --> 1D vector batch_size x time_index.\n",
    "#         # logits.shape[-1] --> vocab_size + 1 --> pops out as the last dimension. First 2D are batch_size x time_index\n",
    "#         loss = None\n",
    "#         if targets is not None:\n",
    "#             loss = F.cross_entropy(logits.view(-1,logits.shape[-1]), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "#         return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    takes the previous block_size tokens, encodes them with a lookup table,\n",
    "    concatenates the vectors and predicts the next token with an MLP.\n",
    "\n",
    "    Reference:\n",
    "    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = max_length + 1\n",
    "        self.vocab_size = vocab_size\n",
    "        self.wte = nn.Embedding(vocab_size + 1,  embedding_dimension) # token embeddings table\n",
    "        # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token\n",
    "        # before the beginning of the input sequence\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.block_size * embedding_dimension, hidden_dimension),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dimension, self.vocab_size)\n",
    "        )\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # print('idx0:', idx)\n",
    "        # print('targets:', targets)\n",
    "        # gather the word embeddings of the previous 3 words - No actually this uses entire sequence\n",
    "        embs = []\n",
    "        for k in range(self.block_size):\n",
    "            tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "            idx = torch.roll(idx, 1, 1)\n",
    "            idx[:, 0] = self.vocab_size # special <BLANK> token\n",
    "            embs.append(tok_emb)\n",
    "            # print(f'idx{k}:', idx)\n",
    "\n",
    "        # print('block_size:', self.block_size)\n",
    "        # print('k:', k)\n",
    "        # print('idx:', idx)\n",
    "        # print(embs)\n",
    "        # sys.exit(1)\n",
    "\n",
    "        # concat all of the embeddings together and pass through an MLP\n",
    "        x = torch.cat(embs, -1) # x is (b, t, n_embd * block_size) \n",
    "        logits = self.mlp(x) # logits are (b, t, vocab_size)\n",
    "        # print('logits shape:', logits.shape)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a small batch to make sure model is working as expected (loss of ~3.4 is consistent with what you would expect ballpark -log(1/27.) ~ 3.3 )\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "xb, yb = next(iter(train_dataloader))\n",
    "# print(xb)\n",
    "# print(yb)\n",
    "\n",
    "# Init the model\n",
    "model = MLP(vocab_size)\n",
    "logits, loss = model(xb,yb)\n",
    "# print(loss)\n",
    "# print('xb:', xb)\n",
    "# print(xb.shape)\n",
    "# print('tok_emb:', tok_emb)\n",
    "# print('tok_emb shape:', tok_emb.shape)\n",
    "# print('embs:', embs)\n",
    "# print('embs shape:', embs.shape)\n",
    "# print(logits)\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "# print(loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write a learning rate inspection function\n",
    "## Add batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and model\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "model = MLP(vocab_size)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_dataloader))\n",
    "logits, loss = model(xb,yb)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimal lr sweep\n",
    "import matplotlib.pyplot as plt\n",
    "lri, lossi =  get_lr_loss(model, optimizer, train_dataloader, batch_size, 500, -2, -.8)\n",
    "plt.plot(lri, lossi)\n",
    "# Add labels to the x-axis and y-axis\n",
    "plt.xlabel('LR (Learning Rate)')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop \n",
    "lr = 0.001\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "tr_loss = []\n",
    "te_loss = []\n",
    "\n",
    "for steps in range(10000):\n",
    "    xb, yb = next(iter(train_dataloader))\n",
    "    logits, loss = model(xb,yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ltr, lte = evaluate_loss(model, train_dataloader, test_dataloader, num_batches=10)\n",
    "    tr_loss.append(ltr)\n",
    "    te_loss.append(lte)\n",
    "    # print(loss.item())\n",
    "\n",
    "    # if steps % 99 == 0:\n",
    "    #     print('ltr: ', ltr, 'lte: ', lte, 'single shot loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot loss \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(tr_loss[1000:])\n",
    "plt.plot(te_loss[1000:])\n",
    "\n",
    "plt.show()\n",
    "print(torch.mean(torch.tensor(tr_loss[-100:])), torch.mean(torch.tensor(te_loss[-100:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pretty print the samples\n",
    "print_samples(model, train_data, max_length, num=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = model.wte.weight.data.cpu().numpy()\n",
    "# embeddings_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot tsne or pca for embeddings\n",
    "# t-SNE example:\n",
    "embeddings_matrix = model.wte.weight.data.cpu().numpy()\n",
    "tsne = TSNE(n_components=2, perplexity=5)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings_matrix)\n",
    "\n",
    "# Create vectors for ticklabels\n",
    "ticklabels = list(train_data.itos.values())\n",
    "ticklabels.insert(0,'stop')\n",
    "ticklabels.insert(27,'blank')\n",
    "x_ticklabel_vec = np.arange(len(ticklabels))\n",
    "y_ticklabel_vec = np.arange(len(ticklabels))\n",
    "print(ticklabels)\n",
    "\n",
    "# visualize dimensions 0 and 1 of the embedding matrix for all characters\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(reduced_embeddings[:,0].data, reduced_embeddings[:,1].data, s=200)\n",
    "for i in range(embeddings_matrix.shape[0]):\n",
    "    plt.text(reduced_embeddings[i,0].item(), reduced_embeddings[i,1].item(), ticklabels[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "### Subjective appraisal\n",
    "1. Better names\n",
    "### TSNE\n",
    "1. Vowels get separated and for max_length we see a lot more structure \n",
    "2. 'Stop' and 'Blank' clustered with consonants\n",
    "3. Loss is dependent on block length - In 5000 epochs of training we get average loss for last 100 losses - for block_length 3 we get to te ~ 2.3, with 8 we get to te ~2.5 for max_length we get 2.5. This could just mean that we need to train for a lot longer with bigger block_length\n",
    "4. After training longer context model its clear that it bottoms out on loss at ~2.4 but tsne plot gets a lot clearer structure. \n",
    "5. Further training with a smalller learning rate reduces loss to ~ 2.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
