{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/Users/shwetank/code/makemore-utils-nbs')\n",
    "from utils import create_dataset, CharDataset, evaluate_loss, print_samples, get_lr_loss\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and create training and test sets\n",
    "names, vocab, max_length = create_dataset('../names.txt')\n",
    "# print(len(names), vocab, max_length)\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "n1 = int(0.9*len(names))\n",
    "random.shuffle(names)\n",
    "# print(names[0:5], names_ss[0:5]) # check that dataset is shuffled\n",
    "train_data = CharDataset(names[:n1], vocab=vocab, max_length=max_length)\n",
    "test_data = CharDataset(names[n1:], vocab=vocab, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, emb_dim, head_size, block_length):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        # Define a register buffer\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_length,block_length)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        ## Initialize the vector\n",
    "        k = self.key(x) # B,T,head_size\n",
    "        q = self.query(x) # B,T,head_size\n",
    "        v = self.value(x) # B,T,head_size\n",
    "        wei = k @ torch.transpose(q, 1, 2) * C**-0.5 # B,T,head_size * B,head_size,T -> B,T,T\n",
    "        wei = torch.masked_fill(wei, self.tril[:T,:T] == 0, float('-Inf')) # Only selecting till the Tth column will be esp important during generation o/w will expect maxx_length columns at everytime step and throw an error\n",
    "        wei = F.softmax(wei, dim=-1) # B,T,T\n",
    "        # print(k.shape, q.shape, v.shape, wei.shape)\n",
    "        out = wei @ v #B,T,T * B,T,H -> B,T,H i.e. 32,16,16 * 32,16,8 -> 32,16,8\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_size, num_heads, emb_dim, block_length):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(emb_dim, head_size, block_length) for i in range(num_heads)]) # B,T,head_size*num_heads\n",
    "\n",
    "    def forward(self,x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, head_size, block_length):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size + 1, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(vocab_size + 1, emb_dim)\n",
    "        self.sa_head = MultiHeadAttention(emb_dim//4, num_heads=4)\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.pos_embedding(x)\n",
    "        x = tok_emb + pos_emb # B, T, emb_dim\n",
    "        x = self.sa_head(x) # B, T, head_size\n",
    "        logits = self.lm_head(x) # B, T, vocab_size\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # print(logits.view(-1, logits.size(-1)).shape, targets.view(-1).shape)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return(logits,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Hyperparameters\n",
    "block_length = max_length + 1 \n",
    "vocab_size = len(vocab) + 1\n",
    "emb_dim = 32\n",
    "head_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop \n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test run on a batch size of 1 for debug\n",
    "# model = Xformer(vocab_size, emb_dim, head_size, block_length)\n",
    "# optimizer = Adam(model.parameters(), lr=0.01)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# xb, yb = next(iter(train_dataloader))\n",
    "# # print(xb.shape, yb.shape)\n",
    "# logits, loss = model(xb,yb)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Head.__init__() missing 2 required positional arguments: 'head_size' and 'block_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      3\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mXformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m, in \u001b[0;36mXformer.__init__\u001b[0;34m(self, vocab_size, emb_dim, head_size, block_length)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, emb_dim)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, emb_dim)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_head \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb_dim\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(emb_dim, vocab_size)\n",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m, in \u001b[0;36mMultiHeadAttention.__init__\u001b[0;34m(self, head_size, num_heads)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, head_size, num_heads):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([Head(head_size) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)])\n",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, head_size, num_heads):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mHead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_size\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)])\n",
      "\u001b[0;31mTypeError\u001b[0m: Head.__init__() missing 2 required positional arguments: 'head_size' and 'block_length'"
     ]
    }
   ],
   "source": [
    "# Set up optimizer and model\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "model = Xformer(vocab_size, emb_dim, head_size, block_length)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_dataloader))\n",
    "logits, loss = model(xb,yb)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimal lr sweep\n",
    "import matplotlib.pyplot as plt\n",
    "lri, lossi =  get_lr_loss(model, optimizer, train_dataloader, batch_size, 500, -2, -1)\n",
    "plt.plot(lri, lossi)\n",
    "# Add labels to the x-axis and y-axis\n",
    "plt.xlabel('LR (Learning Rate)')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop - Initialized in separate loop so that it can be re run without reinitialization that will wipe things out\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "tr_loss = []\n",
    "te_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop \n",
    "lr = 0.001\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "n_epochs = 1000\n",
    "for steps in range(n_epochs):\n",
    "    xb, yb = next(iter(train_dataloader))\n",
    "    logits, loss = model(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    tr_lossi, te_lossi = evaluate_loss(model, train_dataloader, test_dataloader, num_batches=10)\n",
    "    tr_loss.append(tr_lossi)\n",
    "    te_loss.append(te_lossi)\n",
    "    # print(loss.item())\n",
    "\n",
    "    # if steps % 99 == 0:\n",
    "    #     print('ltr: ', ltr, 'lte: ', lte, 'single shot loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot loss \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(tr_loss[10:])\n",
    "plt.plot(te_loss[10:])\n",
    "\n",
    "plt.show()\n",
    "print('training loss: ', round(torch.mean(torch.tensor(tr_loss[-100:])).item(),4)), \n",
    "print('validation loss: ', round((torch.mean(torch.tensor(te_loss[-100:]))).item(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pretty print the samples\n",
    "print_samples(model, train_data, max_length, num=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot tsne or pca for embeddings\n",
    "# t-SNE example:\n",
    "embeddings_matrix = model.token_embedding.weight.data.cpu().numpy()\n",
    "tsne = TSNE(n_components=2, perplexity=5)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings_matrix)\n",
    "\n",
    "# Create vectors for ticklabels\n",
    "ticklabels = list(train_data.itos.values())\n",
    "ticklabels.insert(0,'stop')\n",
    "ticklabels.insert(27,'blank')\n",
    "x_ticklabel_vec = np.arange(len(ticklabels))\n",
    "y_ticklabel_vec = np.arange(len(ticklabels))\n",
    "print(ticklabels)\n",
    "\n",
    "# visualize dimensions 0 and 1 of the embedding matrix for all characters\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(reduced_embeddings[:,0].data, reduced_embeddings[:,1].data, s=200)\n",
    "for i in range(embeddings_matrix.shape[0]):\n",
    "    plt.text(reduced_embeddings[i,0].item(), reduced_embeddings[i,1].item(), ticklabels[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
