{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Read file and create vocab\n",
    "names = open('../names.txt').read().splitlines()\n",
    "vocab = sorted(list(set(''.join(names))))\n",
    "max_name_length = max(len(n) for n in names)\n",
    "print(max_name_length)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Create simple tokenizer (tokenizes one name at a time)\n",
    "stoi = {s: i+1 for i,s in enumerate(vocab)}\n",
    "itos = {s: i for i,s in stoi.items()}\n",
    "encode = lambda name: [stoi[n] for n in name]\n",
    "decode = lambda tokens: ''.join(itos[t] for t in tokens)\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "import random\n",
    "# Set the seed\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "n1 = int(0.9*len(names))\n",
    "random.shuffle(names)\n",
    "# print(names[0:5], names_ss[0:5]) # check that dataset is shuffled\n",
    "train_data = names[:n1]\n",
    "test_data = names[n1:] # there should be a validation dataset to train hyperparameters but this is meant to be quick and dirty\n",
    "# print(len(train_data), len(test_data))\n",
    "encoded_train_data = [encode(name) for name in train_data]\n",
    "encoded_test_data = [encode(name) for name in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get batch of certain sizes\n",
    "def get_batch(batch_size, train=True):\n",
    "    x = torch.zeros(batch_size, max_name_length+1, dtype=torch.long) # +1 to account for start character\n",
    "    y = torch.zeros(batch_size, max_name_length+1, dtype=torch.long) # +1 to account for start character\n",
    "    if train:\n",
    "        encoded_batch_names = random.sample(encoded_train_data, batch_size)\n",
    "    else:\n",
    "        encoded_batch_names = random.sample(encoded_test_data, batch_size)\n",
    "        \n",
    "    encoded_batch_names = random.sample(encoded_test_data, batch_size)\n",
    "    decoded_batch_names = [decode(n) for n in encoded_batch_names]\n",
    "    for idx,e_name in enumerate(encoded_batch_names):\n",
    "        x[idx, 1:1+len(e_name)] = torch.tensor(e_name)\n",
    "        y[idx, :len(e_name)] = torch.tensor(e_name)\n",
    "        y[idx, len(e_name)+1:] = -1 \n",
    "\n",
    "    return x, y #for checking that we are not getting garbage: decoded_batch_names, encoded_batch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking batchifying\n",
    "# x, y, decoded_batch, encoded_batch = get_batch(4)\n",
    "# print(x)\n",
    "# print(y)\n",
    "# print(decoded_batch)\n",
    "# print(encoded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super(Bigram, self).__init__()\n",
    "        self.bigram_embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets = None):\n",
    "        logits = self.bigram_embedding(x) # Outputs Batch, Time, Channel (Vocab Size) \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            self.B,self.T,self.C = logits.shape\n",
    "            logits = logits.view(self.B*self.T,self.C)\n",
    "            targets = targets.contiguous().view(self.B*self.T)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=-1)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    # \"\"\"\n",
    "    # Bigram Language Model 'neural net', simply a lookup table of logits for the\n",
    "    # next character given a previous character.\n",
    "    # \"\"\"\n",
    "\n",
    "    # def __init__(self, vocab_size):\n",
    "    #     super().__init__()\n",
    "    #     n = vocab_size\n",
    "    #     self.logits = nn.Parameter(torch.zeros((n, n)))\n",
    "\n",
    "    # def get_block_size(self):\n",
    "    #     return 1 # this model only needs one previous character to predict the next\n",
    "\n",
    "    # def forward(self, idx, targets=None):\n",
    "\n",
    "    #      # 'forward pass', lol\n",
    "    #     logits = self.logits[idx]\n",
    "\n",
    "    #     # if we are given some desired targets also calculate the loss\n",
    "    #     loss = None\n",
    "    #     if targets is not None:\n",
    "    #         loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "    #     return logits, loss\n",
    "    \n",
    "    # @torch.no_grad()\n",
    "    # def generate(self, x, max_new_tokens):\n",
    "    #     # x should have dimensions (B,T*C) where B is number of new names and T is time dimension (so just start character)\n",
    "    #     # For names we will truncate anything new after the stop character\n",
    "    #     block_size = 1\n",
    "    #     for _ in range(max_new_tokens):\n",
    "    #         x_cond = x if x.size(1) <= block_size else x[:, -block_size:]\n",
    "    #         logits, _ = self(x_cond)\n",
    "    #         # Take softmax along the channel dimension \n",
    "    #         probs = F.softmax(logits, dim=-1)\n",
    "    #         # Sample from the distribution\n",
    "    #         x_next = torch.multinomial(probs, num_samples = 1)\n",
    "    #         # _, x_next = torch.topk(probs, k=1, dim=-1)\n",
    "    #         x = torch.cat((x, x_next[0])) # x dimension B, T --> B, T+1\n",
    "            \n",
    "    #     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check how the neural network is initializing\n",
    "# xb, yb = get_batch(batch_size=4)\n",
    "# vocab_size = len(vocab)\n",
    "# model = Bigram(vocab_size)\n",
    "# logits, loss = model(xb, yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer\n",
    "vocab_size = len(vocab) + 1 # +1 for special characters\n",
    "model = Bigram(vocab_size)\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_loss(num_batches = 10):\n",
    "    model.eval()\n",
    "    loss_tr = []\n",
    "    loss_te = []\n",
    "    for n in range(num_batches):\n",
    "        Xtr, Ytr = get_batch(batch_size)\n",
    "        Xte, Yte = get_batch(batch_size, train=False)\n",
    "        _, train_loss = model(Xtr, Ytr)\n",
    "        _, test_loss = model(Xte, Yte)\n",
    "        loss_tr.append(train_loss)\n",
    "        loss_te.append(test_loss)\n",
    "    \n",
    "    mean_train_loss = torch.tensor(loss_tr).mean().item()\n",
    "    mean_test_loss = torch.tensor(loss_te).mean().item()\n",
    "    model.train()\n",
    "    return(mean_train_loss, mean_test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 19,  9, 12, 22,  1, 14,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0, 20,  1, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0, 11,  1,  4,  1, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0, 25, 21, 24, 21,  1, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0, 11, 15, 21, 18, 20, 14,  5, 25,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  1, 13,  1,  1, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0, 13,  9,  3,  8, 15, 14, 14,  5,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0, 18, 25,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([[19,  9, 12, 22,  1, 14,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [20,  1, 13,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [11,  1,  4,  1, 18,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [25, 21, 24, 21,  1, 14,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [11, 15, 21, 18, 20, 14,  5, 25,  0, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 1, 13,  1,  1, 12,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [13,  9,  3,  8, 15, 14, 14,  5,  0, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [18, 25,  5,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])\n",
      "tensor([[ 0,  3,  1, 18, 15, 12, 25, 14, 14,  5,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0, 23,  9, 12,  9,  1, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0, 20,  9, 13,  9, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0, 19, 21, 19,  1, 14, 14,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  4,  5, 19,  8, 15, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0, 19,  8,  9, 22,  1, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  9, 14,  4,  5,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  2,  5, 14, 14, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([[ 3,  1, 18, 15, 12, 25, 14, 14,  5,  0, -1, -1, -1, -1, -1, -1],\n",
      "        [23,  9, 12,  9,  1, 13,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [20,  9, 13,  9, 18,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [19, 21, 19,  1, 14, 14,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 4,  5, 19,  8, 15, 14,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [19,  8,  9, 22,  1, 25,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 9, 14,  4,  5,  5,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 2,  5, 14, 14, 25,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])\n"
     ]
    }
   ],
   "source": [
    "# Training loop - update to evaluate loss on ongoing basis and printing results, understand whats going on in the bigram model - ie why isnt it working with embedding \n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "tr_loss = []\n",
    "te_loss = []\n",
    "\n",
    "for steps in range(2):\n",
    "    xb, yb = get_batch(batch_size)\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    logits, loss = model(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ltr, lte = evaluate_loss()\n",
    "    tr_loss.append(ltr)\n",
    "    te_loss.append(lte)\n",
    "\n",
    "    # if steps % 99 == 0:\n",
    "    #     print('ltr: ', ltr, 'lte: ', lte, 'single shot loss:', loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(tr_loss)\n",
    "plt.plot(te_loss)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Infer from the model\n",
    "# generated_names = decode(model.generate(torch.tensor([0]),max_new_tokens=10).tolist()[1:]) # We want to crop out the first letter which is a start token\n",
    "# generated_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, block_size=1):\n",
    "    for _ in range(max_name_length):\n",
    "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        # input_tokens = input_tokens[-block_length:] if len(input_tokens) > block_length else input_tokens\n",
    "        # logits, _ = model(input_tokens)\n",
    "        logits = torch.squeeze(logits)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # ouput_token = torch.squeeze(torch.multinomial(probs, 1))\n",
    "        # x.append(ouput_token.tolist())\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_samples(num=10):\n",
    "    \"\"\" samples from the model and pretty prints the decoded samples \"\"\"\n",
    "    X_init = torch.zeros(num, 1, dtype=torch.long)\n",
    "    steps = max_name_length # we already start with <START> token (index 0) so its just len(vocab) instead of len(vocab) + 1\n",
    "    X_samp = generate(model, X_init)[:,1:].tolist()\n",
    "    # print(X_samp)\n",
    "    for row in X_samp:\n",
    "        crop_index = row.index(0) if 0 in row else len(row)\n",
    "        # print(row, crop_index)\n",
    "        row = row[:crop_index]\n",
    "        print(decode(row))\n",
    "    \n",
    "    \n",
    "    # train_samples, test_samples, new_samples = [], [], []\n",
    "    # for i in range(X_samp.size(0)):\n",
    "    #     # get the i'th row of sampled integers, as python list\n",
    "    #     row = X_samp[i, 1:].tolist() # note: we need to crop out the first <START> token\n",
    "    #     # token 0 is the <STOP> token, so we crop the output sequence at that point\n",
    "    #     crop_index = row.index(0) if 0 in row else len(row)\n",
    "    #     row = row[:crop_index]\n",
    "    #     word_samp = decode(row)\n",
    "    #     # separately track samples that we have and have not seen before\n",
    "    #     # if train_dataset.contains(word_samp):\n",
    "    #     #     train_samples.append(word_samp)\n",
    "    #     # elif test_dataset.contains(word_samp):\n",
    "    #     #     test_samples.append(word_samp)\n",
    "    #     # else:\n",
    "    #     #     new_samples.append(word_samp)\n",
    "    #     new_samples.append(word_samp)\n",
    "    # print('-'*80)\n",
    "    # for lst, desc in [(train_samples, 'in train'), (test_samples, 'in test'), (new_samples, 'new')]:\n",
    "    #     print(f\"{len(lst)} samples that are {desc}:\")\n",
    "    #     for word in lst:\n",
    "    #         print(word)\n",
    "    # print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jal\n",
      "kh\n",
      "veyeireneietabh\n",
      "lyl\n",
      "lalle\n",
      "wjonanoviangnck\n",
      "ikepanelemirquk\n",
      "zfr\n",
      "katadr\n",
      "liyquekacaelo\n"
     ]
    }
   ],
   "source": [
    "print_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/shwetank/code/makemore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from makemore import create_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, te = create_datasets('../names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr[5]\n",
    "# tr.decode(tr[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# helper functions for creating the training and test Datasets that emit words\n",
    "from torch.utils.data import Dataset\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, words, chars, max_word_length):\n",
    "        self.words = words\n",
    "        self.chars = chars\n",
    "        self.max_word_length = max_word_length\n",
    "        self.stoi = {ch:i+1 for i,ch in enumerate(chars)}\n",
    "        self.itos = {i:s for s,i in self.stoi.items()} # inverse mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def contains(self, word):\n",
    "        return word in self.words\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.chars) + 1 # all the possible characters and special 0 token\n",
    "\n",
    "    def get_output_length(self):\n",
    "        return self.max_word_length + 1 # <START> token followed by words\n",
    "\n",
    "    def encode(self, word):\n",
    "        ix = torch.tensor([self.stoi[w] for w in word], dtype=torch.long)\n",
    "        return ix\n",
    "\n",
    "    def decode(self, ix):\n",
    "        word = ''.join(self.itos[i] for i in ix)\n",
    "        return word\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        ix = self.encode(word)\n",
    "        x = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        y = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        x[1:1+len(ix)] = ix\n",
    "        y[:len(ix)] = ix\n",
    "        y[len(ix)+1:] = -1 # index -1 will mask the loss at the inactive locations\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(input_file):\n",
    "\n",
    "    # preprocessing of the input text file\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = f.read()\n",
    "    words = data.splitlines()\n",
    "    words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "    words = [w for w in words if w] # get rid of any empty strings\n",
    "    chars = sorted(list(set(''.join(words)))) # all the possible characters\n",
    "    max_word_length = max(len(w) for w in words)\n",
    "    print(f\"number of examples in the dataset: {len(words)}\")\n",
    "    print(f\"max word length: {max_word_length}\")\n",
    "    print(f\"number of unique characters in the vocabulary: {len(chars)}\")\n",
    "    print(\"vocabulary:\")\n",
    "    print(''.join(chars))\n",
    "\n",
    "    # partition the input data into a training and the test set\n",
    "    test_set_size = min(1000, int(len(words) * 0.1)) # 10% of the training set, or up to 1000 examples\n",
    "    rp = torch.randperm(len(words)).tolist()\n",
    "    train_words = [words[i] for i in rp[:-test_set_size]]\n",
    "    test_words = [words[i] for i in rp[-test_set_size:]]\n",
    "    print(f\"split up the dataset into {len(train_words)} training examples and {len(test_words)} test examples\")\n",
    "\n",
    "    # wrap in dataset objects\n",
    "    train_dataset = CharDataset(train_words, chars, max_word_length)\n",
    "    test_dataset = CharDataset(test_words, chars, max_word_length)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = create_datasets('../names.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "class InfiniteDataLoader:\n",
    "    \"\"\"\n",
    "    this is really hacky and I'm not proud of it, but there doesn't seem to be\n",
    "    a better way in PyTorch to just create an infinite dataloader?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        train_sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10))\n",
    "        self.train_loader = DataLoader(dataset, sampler=train_sampler, **kwargs)\n",
    "        self.data_iter = iter(self.train_loader)\n",
    "\n",
    "    def next(self):\n",
    "        try:\n",
    "            batch = next(self.data_iter)\n",
    "        except StopIteration: # this will technically only happen after 1e10 samples... (i.e. basically never)\n",
    "            self.data_iter = iter(self.train_loader)\n",
    "            batch = next(self.data_iter)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = create_datasets('../names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init dataloader\n",
    "batch_loader = InfiniteDataLoader(train_dataset, batch_size=5, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch_loader.next()\n",
    "batch = [t for t in batch]\n",
    "X, Y = batch\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example training data with variable-length sequences\n",
    "# training_data = ['hello', 'world', 'python', 'example', 'abc', 'xyz', 'encompass']\n",
    "# encoded_data = [encode(name) for name in training_data]\n",
    "\n",
    "# # Find the maximum sequence length\n",
    "# block_size = max(len(seq) for seq in encoded_data)\n",
    "\n",
    "# # Pad sequences to the maximum length\n",
    "# padded_data = [seq + [0] * (block_size - len(seq)) for seq in encoded_data]\n",
    "\n",
    "# padded_data_tensor = torch.tensor(padded_data)\n",
    "# x  = padded_data_tensor[:,:block_size-1]\n",
    "# y  = padded_data_tensor[:,1:block_size]\n",
    "# padded_data_tensor,x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        # self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        # self.transformer_decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)\n",
    "        # self.transformer_decoder = nn.TransformerDecoder(self.transformer_decoder_layer, num_layers=num_layers)\n",
    "        # self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.bigram_embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets = None):\n",
    "        logits = self.bigram_embedding(x) # Outputs Batch, Time, Channel (Vocab Size) \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            self.B,self.T,self.C = logits.shape\n",
    "            logits = logits.view(self.B*self.T,self.C)\n",
    "            targets = targets.contiguous().view(self.B*self.T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x, ntokens):\n",
    "        # x should have dimensions (B,T*C) where B is number of new names and T is time dimension (so just start character)\n",
    "        # For names we will truncate anything new after the stop character\n",
    "        for n in range(ntokens):\n",
    "            logits, loss = self(x)\n",
    "            # Take softmax along the channel dimension \n",
    "            probs = F.softmax(logits, -1)\n",
    "            # Sample from the distribution\n",
    "            x_next = torch.multinomial(probs, num_samples = 1)\n",
    "            x = torch.cat((x, x_next[0])) # x dimension B, T --> B, T+1\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "model = TransformerDecoder(vocab_size)\n",
    "logits, loss = model(x, y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "# output.shape # Shape is Batch, Time (including padding), Channel (Vocab size) of 28\n",
    "# print(logits.view(7,8,-1)[:,-1,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print whatever we get out of untrained model\n",
    "x = torch.zeros(1,1,dtype=torch.long)\n",
    "print(decode(model.generate(x[:,0], ntokens=30).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch(batch_size=32)\n",
    "    logits, loss = model(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(model.generate(torch.tensor([0]),ntokens=17).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "d_model = 2 # Start with 2D to see if its separating vowels and consonants\n",
    "nhead = 2 # What does this do?\n",
    "dim_feedforward = 200\n",
    "num_layers = 1\n",
    "model = TransformerDecoder(vocab_size, d_model, nhead, dim_feedforward, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[ 6, 13, 13, 16,  0,  0,  0,  0],\n",
    "        [16, 19, 13,  5,  0,  0,  0,  0],\n",
    "        [26, 21,  9, 16, 15,  0,  0,  0],\n",
    "        [25,  2, 14, 17, 13,  6,  0,  0],\n",
    "        [ 3,  4,  0,  0,  0,  0,  0,  0],\n",
    "        [26, 27,  0,  0,  0,  0,  0,  0],\n",
    "        [15,  4, 16, 14, 17,  2, 20, 20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory_mask = torch.triu(torch.ones((len(encoded_sequence)-1, len(encoded_sequence)-1)), diagonal=1).byte()\n",
    "# memory_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for encoded_sequence in padded_data:\n",
    "        input_sequence = torch.tensor(encoded_sequence[:-1])  # Input sequence excluding the last token\n",
    "        target_sequence = torch.tensor(encoded_sequence[1:])   # Target sequence excluding the first token\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # No memory for a decoder-only model\n",
    "        # memory_mask = torch.triu(torch.ones((len(encoded_sequence)-1, len(encoded_sequence)-1)), diagonal=1).byte()\n",
    "\n",
    "        # output = model(input_sequence.unsqueeze(0), memory_mask=memory_mask)  # Adjust the input shape based on your model requirements\n",
    "        output = model(input_sequence.unsqueeze(0))\n",
    "        loss = criterion(output.squeeze(0), target_sequence)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(padded_data)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a TransformerDecoderBlock\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "        self.transformer_decoder_block = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        return self.transformer_decoder_block(x, memory)\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have an input tensor `input_tensor` of shape (seq_len, batch_size, d_model)\n",
    "seq_len, batch_size, d_model = 10, 3, 512\n",
    "\n",
    "# Initialize the TransformerDecoderBlock\n",
    "decoder_block = TransformerDecoderBlock(d_model=d_model, nhead=8, dim_feedforward=2048)\n",
    "\n",
    "# Generate random input tensor (you would replace this with your actual input)\n",
    "input_tensor = torch.randn(seq_len, batch_size, d_model)\n",
    "\n",
    "# Generate random memory tensor (you would replace this with the encoder output)\n",
    "memory_tensor = torch.randn(seq_len, batch_size, d_model)\n",
    "\n",
    "# Forward pass through the TransformerDecoderBlock\n",
    "output_tensor = decoder_block(input_tensor, memory_tensor)\n",
    "\n",
    "# Print the shapes\n",
    "print(\"Input Tensor Shape:\", input_tensor.shape)\n",
    "print(\"Output Tensor Shape:\", output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training source and targets\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t]\n",
    "    target = y[t+1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 3\n",
    "\n",
    "def build_dataset(names):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for n in names:\n",
    "        # print(n)\n",
    "        context = [0]*context_length\n",
    "        for ch in n + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itos[i] for i in context), '--->',itos[ix])\n",
    "            context = context[1:] + [ix]      \n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "len(names)\n",
    "random.seed(42)\n",
    "n1 = int(len(names)*0.8)\n",
    "n2 = int(len(names)*0.9)\n",
    "random.shuffle(names)\n",
    "Xtr, Ytr = build_dataset(names[:n1])\n",
    "Xval, Yval = build_dataset(names[n1:n2])\n",
    "Xte, Yte = build_dataset(names[n2:])\n",
    "print(\"Training Set Size:\", \"\\n\",\"Xtr:\", Xtr.shape,\"\\n\",\"Ytr:\",Ytr.shape)\n",
    "print(\"Validation Set Size:\", \"\\n\",\"Xval:\", Xval.shape,\"\\n\",\"Yval:\",Yval.shape)\n",
    "print(\"Test Set Size:\", \"\\n\",\"Xte:\", Xte.shape,\"\\n\",\"Yte:\",Yte.shape)\n",
    "# print(Xval.shape, Yval.shape)\n",
    "# print(Xte.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_type = 'Transformer'\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # Layers\n",
    "        self.positional_encoding = PositionalEncoding(dim_model=dim_model, dropout_p=dropout_p , max_len=8)\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(dim_model, num_tokens)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src)*math.sqrt(self.dim_model) \n",
    "        tgt = self.embedding(tgt)*math.sqrt(self.dim_model) \n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        transformer_out = self.transformer(src, tgt)\n",
    "        out = self.out(transformer_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_tokens = len(vocab)\n",
    "dim_model = 12\n",
    "num_heads = 4\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 1\n",
    "dropout_p = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate and batch size experiment\n",
    "num_epochs = 512\n",
    "batch_size = 1024\n",
    "\n",
    "lrexp = torch.linspace(-3,0.01,num_epochs, requires_grad=False)\n",
    "lrs_val = 10**lrexp\n",
    "\n",
    "lri = []\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network, loss and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "model = Transformer(num_of_tokens, dim_model, num_heads, num_encoder_layers, num_decoder_layers, dropout_p)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lrs_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with mini-batches and lr sweep\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    ## Set learning rate\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lrs_val[epoch]\n",
    "\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "\n",
    "    inputs = Xtr[ix]\n",
    "    labels = Ytr[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs, labels)\n",
    "    loss = loss_function(outputs, labels)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    lri.append(lrs_val[epoch])\n",
    "    lossi.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
