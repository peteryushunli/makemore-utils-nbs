{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/Users/shwetank/code/makemore-utils-nbs')\n",
    "from utils import create_dataset, CharDataset, evaluate_loss, print_samples\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and create training and test sets\n",
    "names, vocab, max_length = create_dataset('../names.txt')\n",
    "# print(len(names), vocab, max_length)\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "n1 = int(0.9*len(names))\n",
    "random.shuffle(names)\n",
    "# print(names[0:5], names_ss[0:5]) # check that dataset is shuffled\n",
    "train_data = CharDataset(names[:n1], vocab=vocab, max_length=max_length)\n",
    "test_data = CharDataset(names[n1:], vocab=vocab, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super(Bigram, self).__init__()\n",
    "        self.bigram_embedding = nn.Embedding(vocab_size, vocab_size) # Because stop word will never be an input\n",
    "\n",
    "    def forward(self, x, targets = None):\n",
    "        logits = self.bigram_embedding(x) # Outputs Batch, Time, Channel (Vocab Size) \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            self.B,self.T,self.C = logits.shape\n",
    "            logits = logits.view(self.B*self.T,self.C)\n",
    "            targets = targets.contiguous().view(self.B*self.T)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=-1)\n",
    "            \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on model architecture\n",
    "\n",
    "Model has only 1 layer - the embedding layer. These always seems a little magical to me to lets try to understand what it means physically - we will validate this quantitatively as well. The layer is a matrix of vocab_size x vocab_size where vocab_size = 27 (alphabets + stop character). I think of this in 2 ways:\n",
    "\n",
    "1. Less generic - the rows and columns indices of matrix are vocab elements and the matrix cells are the probability that the vocab element indexing the row appears after the vocab element indexing the column. Its basically trying to captured the conditional probability that row element appears given that the column element just appeared\n",
    "2. More generic - ^^ physical interpretation works for this case but more generally we will choose embeddings to have arbitrary vector dimensions which dont have vocab size in one of the dimensions. In this case you can think of each vocab element repesented by an n dimensional vector where each dimension does not need to mean anything specific we can associate with our physical world. (I mean you can always torture a mapping if you want to ...). These vectors then get dotted and operated up by appropriate NN operators to finallly provide the probability of x (where x can be vector respresenting some combination of words) -> results in vocab element y.\n",
    "3. By quantifying we mean we can do 2 things - in this case we can plot the cross correlation matrix between the 2 vocab sizes to see elements likelihood of preceding one another. In more general case (also in this?) we can plot the embeddings in 2 dimensions using PCA or TSNE to see which elements group together and whether that makes sense e.g. I would expect vowels to be close, less common alphabets to be separated out etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer\n",
    "vocab_size = len(vocab) + 1 # +1 for special characters\n",
    "model = Bigram(vocab_size)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop \n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "tr_loss = []\n",
    "te_loss = []\n",
    "\n",
    "for steps in range(5000):\n",
    "    xb, yb = next(iter(train_dataloader))\n",
    "    logits, loss = model(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    tr_lossi, te_lossi = evaluate_loss(model, train_dataloader, test_dataloader, num_batches=10)\n",
    "    tr_loss.append(tr_lossi)\n",
    "    te_loss.append(te_lossi)\n",
    "    # print(loss.item())\n",
    "\n",
    "    # if steps % 99 == 0:\n",
    "    #     print('ltr: ', ltr, 'lte: ', lte, 'single shot loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot loss \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(tr_loss)\n",
    "plt.plot(te_loss)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('training loss: ', round(torch.mean(torch.tensor(tr_loss[-100:])).item(),4)), \n",
    "print('validation loss: ', round((torch.mean(torch.tensor(te_loss[-100:]))).item(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pretty print the samples\n",
    "print_samples(model, train_data, max_length, num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the cross correlation matrix for the embeddings - note that its asymmetric\n",
    "# model.bigram_embedding.weight.shape\n",
    "embeddings_matrix = model.bigram_embedding.weight.data.cpu().numpy()\n",
    "\n",
    "# Create vectors for ticklabels\n",
    "ticklabels = list(train_data.itos.values())\n",
    "ticklabels.insert(0,'stop')\n",
    "x_ticklabel_vec = np.arange(len(ticklabels))\n",
    "y_ticklabel_vec = np.arange(len(ticklabels))\n",
    "print(ticklabels)\n",
    "\n",
    "# Plot the embedding matrix as a 2D matrix plot\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(embeddings_matrix, cmap='Blues', aspect='auto')\n",
    "plt.colorbar(label='Embedding Value')\n",
    "plt.title('2D Matrix Plot of Embeddings')\n",
    "plt.xlabel('Second alphabet')\n",
    "plt.ylabel('Context alphabet')\n",
    "plt.xticks(x_ticklabel_vec, ticklabels)\n",
    "plt.yticks(y_ticklabel_vec, ticklabels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot tsne or pca for embeddings\n",
    "# t-SNE example:\n",
    "tsne = TSNE(n_components=2, perplexity=5)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings_matrix)\n",
    "\n",
    "# visualize dimensions 0 and 1 of the embedding matrix for all characters\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(reduced_embeddings[:,0].data, reduced_embeddings[:,1].data, s=200)\n",
    "for i in range(embeddings_matrix.shape[0]):\n",
    "    plt.text(reduced_embeddings[i,0].item(), reduced_embeddings[i,1].item(), ticklabels[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "### Subjective appraisal\n",
    "1. Not great names being printed - though some are in the realm of reasonable\n",
    "### Embeddings matrix\n",
    "1. Stop by design does not come before any character. Stop comes after all characters\n",
    "2. Vowels all come after various consonants\n",
    "3. 'u' has high likelihood of following 'q'\n",
    "### TSNE\n",
    "1. Vowels do get separated\n",
    "2. 'Stop' gets clustered with vowels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
